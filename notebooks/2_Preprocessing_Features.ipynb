{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Przetwarzanie wstępne",
   "id": "b8cfe415176870c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importy i wczytywanie danych",
   "id": "279171216b72f47b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:28.251246Z",
     "start_time": "2025-12-09T16:31:26.389617Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"sh0416/ag_news\")\n",
    "\n",
    "label2name = {\n",
    "    1: \"World\",\n",
    "    2: \"Sports\",\n",
    "    3: \"Business\",\n",
    "    4: \"Sci/Tech\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Czyszczenie i scalanie tytułu z opisem",
   "id": "16098bc3419b0d36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:28.268476Z",
     "start_time": "2025-12-09T16:31:28.255183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def prepare_text(example):\n",
    "    full_text = example[\"title\"] + \" \" + example[\"description\"]\n",
    "    example[\"text\"] = full_text\n",
    "    example[\"text_clean\"] = clean_text(full_text)\n",
    "    example[\"label_zero_based\"] = example[\"label\"] - 1\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(prepare_text)\n",
    "dataset"
   ],
   "id": "af33ff96b29ae326",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'title', 'description', 'text', 'text_clean', 'label_zero_based'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'title', 'description', 'text', 'text_clean', 'label_zero_based'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:28.734003Z",
     "start_time": "2025-12-09T16:31:28.272771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.15,\n",
    "    stratify=df_train[\"label_zero_based\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "valid_ds = Dataset.from_pandas(valid_df, preserve_index=False)\n",
    "test_ds  = dataset[\"test\"]\n",
    "\n",
    "len(train_ds), len(valid_ds), len(test_ds)\n"
   ],
   "id": "51daf6a2ea61fdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102000, 18000, 7600)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Przygotowanie list i etykiet",
   "id": "b3624942c38f1ec7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:33.702251Z",
     "start_time": "2025-12-09T16:31:28.744944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_text = [ex[\"text_clean\"] for ex in train_ds]\n",
    "y_train      = [ex[\"label_zero_based\"] for ex in train_ds]\n",
    "\n",
    "X_valid_text = [ex[\"text_clean\"] for ex in valid_ds]\n",
    "y_valid      = [ex[\"label_zero_based\"] for ex in valid_ds]\n",
    "\n",
    "X_test_text  = [ex[\"text_clean\"] for ex in test_ds]\n",
    "y_test       = [ex[\"label_zero_based\"] for ex in test_ds]"
   ],
   "id": "78c025f3a89e8d8e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reprezentacja 1: TF-IDF",
   "id": "ba51c925a9ace748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:41.309923Z",
     "start_time": "2025-12-09T16:31:33.719739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_valid_tfidf = tfidf.transform(X_valid_text)\n",
    "X_test_tfidf  = tfidf.transform(X_test_text)\n",
    "\n",
    "X_train_tfidf.shape, X_valid_tfidf.shape, X_test_tfidf.shape\n"
   ],
   "id": "41b953a885b07c0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102000, 20000), (18000, 20000), (7600, 20000))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:41.564666Z",
     "start_time": "2025-12-09T16:31:41.323529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(tfidf, \"../models/tfidf_vectorizer.joblib\")\n",
    "joblib.dump({\n",
    "    \"X_train\": X_train_tfidf,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_valid\": X_valid_tfidf,\n",
    "    \"y_valid\": y_valid,\n",
    "    \"X_test\":  X_test_tfidf,\n",
    "    \"y_test\":  y_test\n",
    "}, \"../data/processed/tfidf_data.joblib\")\n"
   ],
   "id": "37e046b9246b22b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/tfidf_data.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reprezentacja 2: Sekwencje + Embedding",
   "id": "4f799e2da3b85608"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:45.788149Z",
     "start_time": "2025-12-09T16:31:41.572512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_words = 20000\n",
    "max_len   = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_train_text),\n",
    "    maxlen=max_len,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "X_valid_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_valid_text),\n",
    "    maxlen=max_len,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "X_test_seq = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(X_test_text),\n",
    "    maxlen=max_len,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\"\n",
    ")\n",
    "\n",
    "X_train_seq.shape, X_valid_seq.shape, X_test_seq.shape\n"
   ],
   "id": "ceb48c318613776f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102000, 200), (18000, 200), (7600, 200))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:46.415095Z",
     "start_time": "2025-12-09T16:31:45.804259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "joblib.dump(tokenizer, \"../models/tokenizer_keras.joblib\")\n",
    "joblib.dump({\n",
    "    \"X_train_seq\": X_train_seq,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_valid_seq\": X_valid_seq,\n",
    "    \"y_valid\": y_valid,\n",
    "    \"X_test_seq\":  X_test_seq,\n",
    "    \"y_test\":  y_test\n",
    "}, \"../data/processed/sequence_data.joblib\")"
   ],
   "id": "71fea446d525958d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/sequence_data.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Reprezentacja 3: Tokeny transformera",
   "id": "bc8321f0f100ea92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:51.755930Z",
     "start_time": "2025-12-09T16:31:46.423711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return hf_tokenizer(\n",
    "        batch[\"text_clean\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_train = train_ds.map(tokenize_function, batched=True)\n",
    "tokenized_valid = valid_ds.map(tokenize_function, batched=True)\n",
    "tokenized_test  = test_ds.map(tokenize_function,  batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.remove_columns([\"title\", \"description\", \"text\", \"text_clean\", \"label\"])\n",
    "tokenized_valid = tokenized_valid.remove_columns([\"title\", \"description\", \"text\", \"text_clean\", \"label\"])\n",
    "tokenized_test  = tokenized_test.remove_columns([\"title\", \"description\", \"text\", \"text_clean\", \"label\"])\n",
    "\n",
    "tokenized_train = tokenized_train.rename_column(\"label_zero_based\", \"labels\")\n",
    "tokenized_valid = tokenized_valid.rename_column(\"label_zero_based\", \"labels\")\n",
    "tokenized_test  = tokenized_test.rename_column(\"label_zero_based\", \"labels\")\n",
    "\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_valid.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")\n"
   ],
   "id": "17df6081803f144f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 102000/102000 [00:03<00:00, 25734.78 examples/s]\n",
      "Map: 100%|██████████| 18000/18000 [00:00<00:00, 28571.61 examples/s]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T16:31:51.777254Z",
     "start_time": "2025-12-09T16:31:51.768034Z"
    }
   },
   "cell_type": "code",
   "source": "hf_tokenizer.save_pretrained(\"../models/distilbert_tokenizer\")\n",
   "id": "143bdc9a805a0c6c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/distilbert_tokenizer/tokenizer_config.json',\n",
       " '../models/distilbert_tokenizer/special_tokens_map.json',\n",
       " '../models/distilbert_tokenizer/vocab.txt',\n",
       " '../models/distilbert_tokenizer/added_tokens.json',\n",
       " '../models/distilbert_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
